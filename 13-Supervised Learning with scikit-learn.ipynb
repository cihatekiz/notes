{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~the art and science of giving computers the ability to learn to make decisions from data without being explicitly programmed.\n",
    "\n",
    "e.g. your computer can predict whether an email is spam or not   --> there is a label -- called **supervised learning**\n",
    "     your computer can learn to cluster                          --> no label         -- called **unsupervised learning**\n",
    "     \n",
    "~**reinforcement learning** : machines and software agents interact with an environment.\n",
    "                          learn how to optimize their behaviour given a system of rewards and punishments\n",
    "                          draws inspritation from behavioral psychology\n",
    "    *applications are economics,genetics,game playing\n",
    "    \n",
    "**supervised learning**\n",
    "    predictor variables    aim is to predict **the target variable**\n",
    "    features=predictor variable=independent variables\n",
    "    target variable=dependent variable=response variable\n",
    "    classification: target variable consists of categories\n",
    "    regression: target variable is continuous\n",
    "    \n",
    "**scikit-learn/sklearn**  : popular machine learning library for Python. OTHERS : TensorFlow or keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPLORATORY DATA ANALYSIS (EDA)\n",
    "VISUAL EDA\n",
    "**training data** : already called data\n",
    "k-Nearest Neighbors: to predict the label of any data point by looking at the K and taking a majority vote\n",
    "\n",
    "*all machine learning models implemented as Python classes.:\n",
    "    - they implement the algorithms for learning and predicting.\n",
    "    -store the information learned from the data\n",
    " ~training a modal on the data = 'fitting' a model to the data. -->.fit() method -- .predict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**from sklearn.neighbors import KNeighborsClassifier**\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "knn.fit(iris['data'], iris['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.g.\n",
    "~ Import KNeighborsClassifier from sklearn.neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "\n",
    "~Create arrays for the features and the response variable\n",
    "y = df['party'].values\n",
    "X = df.drop('party', axis=1).values\n",
    "~Create a k-NN classifier with 6 neighbors: knn\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "~Fit the classifier to the data\n",
    "knn.fit(X, y)\n",
    "~Predict the labels for the training data X: y_pred\n",
    "y_pred = knn.predict(X)\n",
    "~Predict and print the label for the new data point X_new\n",
    "new_prediction = knn.predict(X_new)\n",
    "print(\"Prediction: {}\".format(new_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MEASURING MODEL PERFORMANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy = fraction of correct predictions\n",
    "~it is common to practice to split your data into two sets, a training and a test set\n",
    "~fit/train the classifier on the training set \n",
    "~make predictions on the test set\n",
    "~compare predictions with the known labels\n",
    "--->**from sklearn.model_selection import train_test_split**\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test =   train_test_split(X, y, test_size=0.3,random_state=21, stratify=y)\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\\\"Test set predictions:\\\\n {}\\\".format(y_pred))\n",
    "knn.score(X_test, y_test)\n",
    "->0.9555555555555556\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION TO REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~ In regression tasks, the target value is a continuously varying variable.\n",
    "\n",
    "X= boston.drop('MEDV', axis=1).values   --> we drop the target\n",
    "y=boston['MEDV'].values                 --> we keep the target\n",
    "\n",
    "\n",
    "X_rooms= X[:,50 ] --> the average number of room\n",
    "y = y.reshape(-1, 1)\n",
    "X_rooms = X_rooms.reshape(-1, 1)\n",
    "plt.scatter(X_rooms, y)\n",
    "plt.ylabel('Value of house /1000 ($)')\n",
    "plt.xlabel('Number of rooms')\n",
    "plt.show();\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "**from sklearn.linear_model import LinearRegression**\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_rooms, y)\n",
    "prediction_space = **np.linspace(min(X_rooms),max(X_rooms)).reshape(-1, 1)**\n",
    "plt.scatter(X_rooms, y, color='blue')\n",
    "plt.plot(prediction_space, reg.predict(prediction_space),color='black',linewidth=3)\n",
    "plt.show()\n",
    "\n",
    "(heatmap func: sns.heatmap(df.corr(), square=True, cmap='RdYlGn')) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~y= ax+b  ; y is the target, x is the single feature and a ,b are the parameters of the model that we want to learn.\n",
    "~y=ax+cx+b  --> when we have two features and one target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~Import LinearRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "~Create the regressor: reg\n",
    "reg = LinearRegression()\n",
    "\n",
    "~Create the prediction space\n",
    "prediction_space = np.linspace(min(X_fertility), max(X_fertility)).reshape(-1,1)\n",
    "\n",
    "~Fit the model to the data\n",
    "reg.fit(X_fertility,y)\n",
    "\n",
    "~Compute predictions over the prediction space: y_pred\n",
    "y_pred = reg.predict(prediction_space)\n",
    "\n",
    "~Print R^2 \n",
    "print(reg.score(X_fertility,y))\n",
    "\n",
    "~Plot regression line\n",
    "plt.plot(prediction_space, y_pred, color='black', linewidth=3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we split the data into 5 different folders\n",
    "metric.1:1st test, 2-5 training\n",
    "metric.2:2nd test, 1,3,4,5 training\n",
    "metric.3:3rd test, 1,2,4,5 training\n",
    "metric.4:4th test, 1,2,3,5 training \n",
    "metric.5:5th test, 1,2,3,4 training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.g.\n",
    "Cross-validation in scikit-learn\n",
    "**from sklearn.model_selection import cross_val_score**\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "cv_results = cross_val_score(reg, X, y, cv=5)\n",
    "print(cv_results)  \n",
    "~[0.63919994  0.71386698  0.58702344  0.07923081 -0.25294154]\n",
    "np.mean(cv_results)\n",
    "~ 0.35327592439587058\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penalizing large coefcients: Regularization  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha: Parameter we need to choose\n",
    "Picking alpha here is similar to picking k in k-NN\n",
    "Hyperparameter tuning (More in Chapter 3)\n",
    "Alpha controls model complexity\n",
    "Alpha = 0: We get back OLS (Can lead to overfitting)\n",
    "Very high alpha: Can lead to underfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge regression in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**from sklearn.linear_model import Ridge**\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size = 0.3, random_state=42)\n",
    "ridge = Ridge(alpha=0.1, normalize=True)\n",
    "ridge.fit(X_train, y_train)\n",
    "ridge_pred = ridge.predict(X_test)\n",
    "ridge.score(X_test, y_test) ~>0.69969382751273179\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso regression in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~it can be used to select important features of a dataset.\n",
    "~it helps us to communicate important results to non-technical colleagues.\n",
    "\n",
    "e.g.\n",
    "**from sklearn.linear_model import Lasso**\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size = 0.3, random_state=42)\n",
    "lasso = Lasso(alpha=0.1, normalize=True)\n",
    "lasso.fit(X_train, y_train)\n",
    "lasso_pred = lasso.predict(X_test)\n",
    "lasso.score(X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.g.-2\n",
    "~Import Lasso\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "~Instantiate a lasso regressor: lasso\n",
    "lasso = Lasso(alpha=0.4, normalize=True)\n",
    "\n",
    "~Fit the regressor to the data\n",
    "lasso.fit(X, y)\n",
    "\n",
    "~Compute and print the coefficients\n",
    "lasso_coef = lasso.coef_\n",
    "print(lasso_coef)\n",
    "\n",
    "~Plot the coefficients\n",
    "plt.plot(range(len(df_columns)), lasso_coef)\n",
    "plt.xticks(range(len(df_columns)), df_columns.values, rotation=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How good is your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy is not always a useful model.\n",
    "spam %1 / if all is correct,accuracy would be 99%, sounds great but a horrible job of predicting spams, \n",
    "called **Class imbalance**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONFUSION MATRIX**\n",
    "~class of interest is positive class in the matrix; the correctly labeled spam mails == true positive\n",
    "                                                  ; the correctly labeled real mails == true negative \n",
    "                                                  ; the incorrectly labeled spam mails == false positive\n",
    "                                                  ; the incorrectly labeled real mails == false negative\n",
    "Metrics from the confusion matrix\n",
    "Precision :tp/(tp+fp)\n",
    "Recall : tp/(tp+fn)  is also called sensitivity/hit rate/ true positive rate.\n",
    "F1score: 2*(precision.recall)/(precision+recall)\n",
    "High precision: Not many real emails predicted as spam // means that the classifiers had a low \"false positive\" rate\n",
    "High recall: Predicted most spam emails correctly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "knn = KNeighborsClassifier(n_neighbors=8)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.4, random_state=42)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "**print(confusion_matrix(y_test, y_pred))**\n",
    ">>[[52  7] [ 3 112]]\n",
    "\n",
    "**print(classification_report(y_test, y_pred))**    \n",
    "->>\n",
    "           precision    recall  f1-score   support          \n",
    "0            0.95       0.88       0.91       59\n",
    "1            0.94       0.97       0.96      115\n",
    "avg/total    0.94       0.94       0.94      174"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is used in classification problems (binary), not regression problems.\n",
    "If the probability ‘p’ is greater than 0.5:The data is labeled ‘1’\n",
    "If the probability ‘p’ is less than 0.5:The data is labeled ‘0’\n",
    "~Logistic regression in scikit-learn\n",
    "**from sklearn.linear_model import LogisticRegression**\n",
    "from sklearn.model_selection import train_test_split\n",
    "logreg = LogisticRegression()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.4, random_state=42)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "\n",
    "**Plotting the ROC(receiver operator characteristic curve)**\n",
    "\n",
    "**from sklearn.metrics import roc_curve**\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label='Logistic Regression')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Logistic Regression ROC Curve')\n",
    "plt.show();\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Area under the ROC curve (AUC)\n",
    "**~Larger area under the ROC curve = better model**\n",
    "\n",
    "**from sklearn.metrics import roc_auc_score**\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.4, random_state=42)\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "-->0.997466216216\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC using cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**from sklearn.model_selection import cross_val_score**\n",
    "cv_scores = cross_val_score(logreg, X, y, cv=5,scoring='roc_auc')\n",
    "print(cv_scores)[ 0.99673203  0 .99183007  0.99583796  1.   0.96140652]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~Linear regression: Choosing parameters\n",
    "~Ridge/lasso regression: Choosing alpha\n",
    "~k-Nearest Neighbors: Choosing n_neighbors\n",
    "~Parameters like alpha and k: Hyperparameters\n",
    "~Hyperparameters cannot be learned by fitting the model\n",
    "\n",
    "CALLED HYPERPARAMETER TUNING\n",
    "\n",
    "To choose the correct hyperparameter\n",
    "    Try a bunch of different hyperparameter values\n",
    "    Fit all of them separately\n",
    "    See how well each performs\n",
    "    Choose the best performing one\n",
    "    It is essential to use cross-validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**from sklearn.model_selection import GridSearchCV**\n",
    "param_grid = {'n_neighbors': np.arange(1, 50)}\n",
    "knn = KNeighborsClassifier()\n",
    "knn_cv = GridSearchCV(knn, param_grid, cv=5)\n",
    "knn_cv.fit(X, y)\n",
    "knn_cv.best_params_{'n_neighbors': 12}\n",
    "knn_cv.best_score_\n",
    "-->0.933216168717"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.4, random_state=42)\n",
    "\n",
    "# Create the hyperparameter grid\n",
    "l1_space = np.linspace(0, 1, 30)\n",
    "param_grid = {'l1_ratio': l1_space}\n",
    "\n",
    "# Instantiate the ElasticNet regressor: elastic_net\n",
    "elastic_net = ElasticNet()\n",
    "\n",
    "# Setup the GridSearchCV object: gm_cv\n",
    "gm_cv =GridSearchCV(elastic_net, param_grid, cv=5)\n",
    "\n",
    "# Fit it to the training data\n",
    "gm_cv.fit(X_train,y_train)\n",
    "\n",
    "# Predict on the test set and compute metrics\n",
    "y_pred = gm_cv.predict(X_test)\n",
    "r2 = gm_cv.score(X_test,y_test)\n",
    "mse = mean_squared_error(y_test,y_pred)\n",
    "print(\"Tuned ElasticNet l1 ratio: {}\".format(gm_cv.best_params_))\n",
    "print(\"Tuned ElasticNet R squared: {}\".format(r2))\n",
    "print(\"Tuned ElasticNet MSE: {}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn will not accept categorical features by default\n",
    "Need to encode categorical features numerically\n",
    "Convert to ‘dummy variables’\n",
    "0: Observation was NOT in that category\n",
    "1: Observation was in that category\n",
    "scikit-learn:OneHotEncoder()\n",
    "pandas:get_dummies()\n",
    "\n",
    "**~Encoding dummy variables~**\n",
    "import pandas as pd\n",
    "df = pd.read_csv('auto.csv')\n",
    "df_origin = pd.get_dummies(df)\n",
    "print(df_origin.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with dummy variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=42)\n",
    "ridge = Ridge(alpha=0.5, normalize=True).fit(X_train,y_train)\n",
    "ridge.score(X_test, y_test)\n",
    "-->0.719064519022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~ missing values can be encoded by zeroes,question marks or negative ones.\n",
    "dropping missing data:\n",
    "    -drop all rows containing missing data\n",
    "    -impute the missing data   ((df.bmi.replace(0, np.nan, inplace=True)))\n",
    "    \n",
    "**from sklearn.preprocessing import Imputer**\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp.fit(X)\n",
    "X = imp.transform(X)\n",
    "or scikit-learn pipeline object "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing within a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**from sklearn.pipeline import Pipeline**\n",
    "**from sklearn.preprocessing import Imputer**\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "logreg = LogisticRegression()\n",
    "steps = [('imputation', imp), ('logistic_regression', logreg)]\n",
    "pipeline = Pipeline(steps)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=42)\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "pipeline.score(X_test, y_test)\n",
    "-->0.75324675324675328"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# centering and scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why scale your data?\n",
    "Many models use some form of distance to inform them\n",
    "Features on larger scales can unduly influence the model\n",
    "Example: k-NN uses distance explicitly when making predictions\n",
    "We want features to be on a similar scaleNormalizing (or scaling and centering)\n",
    "ways \n",
    "Standardization: Subtract the mean and divide by variance\n",
    "    All features are centered around zero and have variance one\n",
    "    Can also subtract the minimum and divide by the range\n",
    "    Minimum zero and maximum one Can also normalize so the data ranges from -1 to +1\n",
    "    See scikit-learn docs for further details\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling in scikit-learn\n",
    "from sklearn.preprocessing import scale\n",
    "X_scaled = scale(X)\n",
    "np.mean(X), np.std(X)\n",
    "-->(8.13421922452, 16.7265339794)\n",
    "np.mean(X_scaled), np.std(X_scaled)\n",
    "-->(2.54662653149e-15, 1.0)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "steps = [('scaler', StandardScaler()),('knn', KNeighborsClassifier())]\n",
    "pipeline = Pipeline(steps)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=21)\n",
    "knn_scaled = pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n",
    "0.956\n",
    "knn_unscaled = KNeighborsClassifier().fit(X_train, y_train)\n",
    "knn_unscaled.score(X_test, y_test)\n",
    "0.928\n",
    "~CV and scaling in a pipeline\n",
    "steps = [('scaler', StandardScaler()),(('knn', KNeighborsClassifier())]\n",
    "pipeline = Pipeline(steps)\n",
    "parameters = {knn__n_neighbors: np.arange(1, 50)}\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=21)\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters)\n",
    "cv.fit(X_train, y_train)\n",
    "y_pred = cv.predict(X_test)                              \n",
    "print(cv.best_params_)\n",
    "print(cv.score(X_test, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
