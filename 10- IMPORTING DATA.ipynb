{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~ filename='sadad.txt'\n",
    "~ file= open(filename,mode='r') / with open('dsadas.txt','r') as file:\n",
    "                                     print(file.read())\n",
    "                                     with is called as context manager\n",
    "~ file.readline()                                    \n",
    "~ text=file.read()\n",
    "~ file.close()\n",
    "\n",
    "~ ! ls -->to display the contents of your current directory\n",
    "\n",
    "~ *flat files*: are basic text files containing records,that is, table data, without structured relationships.\n",
    "   ~record: row of fields or attributes\n",
    "   ~column: feature or attribute\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~ loadtx()       \n",
    "~ data= np.loadtext(filename,delimeter=',' or '\\t',skiprows=1,usecols= [0,2],dtype=str(to ensure that all imported that be string)\n",
    "\n",
    "~ genfromtxt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~ Build a numpy array from the DataFrame: data_array\n",
    "  data_array = data.values\n",
    "  \n",
    "~ import pickle\n",
    "\n",
    "with open('sadaa.pkl','rb') as file:\n",
    "    data=pickle.load(file)\n",
    "    \n",
    "~file='sadas.xlsx'\n",
    "data=pd.ExcelFile(file)\n",
    "print(data.sheet_names)\n",
    "df=data.parse('string of column')\n",
    "df=data.parse(index)\n",
    "\n",
    "~df2 = xls.parse(1, usecols=[0], skiprows=[0], names=['Country'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*SAS= Statistical Analysis System * \n",
    "\n",
    "*business analytics and biostatistics\n",
    "*advanced\n",
    "*multivariate analysis\n",
    "*business intelligence\n",
    "*data management\n",
    "*standard for computational analysis\n",
    "\n",
    "from sas7bdat import SAS7BDAT\n",
    "~'asddad.sas7bcat' -->catalog files \n",
    "~'sadada.sas7bdat' -->dataset files\n",
    "with SAS7BDAT('sales.sas7bdat') as file:\n",
    "  df_sas= file.to_data_frame()\n",
    "\n",
    "*Stata-= 'Statistics' + \"data\"*\n",
    "\n",
    "*academic social social sciences research\n",
    "import pandas as pd\n",
    "data=pd.read_stata('dsadas.dta')\n",
    "\n",
    "\n",
    "*HDF5 == Hierarchial Data Format version 5*\n",
    "\n",
    "~standart for storing large quantities of numerical data\n",
    "~datasets can be hundreds of gigabytes or terabytes\n",
    "\n",
    "   *import h5py\n",
    "    filename='sadfsdfsdfsdfsdf.hddf5'\n",
    "    data=h5py.File(filename,'r')\n",
    "    ~structure~\n",
    "    keys= meta,quality,strain (they are like documentaries)\n",
    "    \n",
    "*MATLAB FILES*\n",
    "~'Matrix Laboratory'\n",
    "~Industry standard in engineering and science\n",
    "~data saved as.mat files\n",
    "scipy.io.loadmat()- read.mat files\n",
    "scipy.io.savemat()- write.mat files\n",
    "        *import scipy.io\n",
    "        filename='dsaddasda.mat'\n",
    "        mat=scipy.io.loadmat(filename)\n",
    "        print(type(mat))\n",
    "~keys= MATLAB variable names\n",
    "~values= objects assigned to variable\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# relational databases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~primary key : to use to explicitly access the row in question.\n",
    "\n",
    "~tables are linked.\n",
    "\n",
    "**RELATIONAL DATABASE MANAGEMENT SYSTEMS**\n",
    "-->PostgreSQL\n",
    "-->MySQL\n",
    "-->SQLite\n",
    "-->SQL = structure query language\n",
    "\n",
    "\n",
    "**Creating a database engine**\n",
    "from sqlalchemy import create_engine\n",
    "engine=create_engine('sqlite:///file.sqlite')\n",
    "getting table names --> engine.table_names()\n",
    "\n",
    "**querying**: getting data out from a database\n",
    "\n",
    "SELECT * FROM Table_name\n",
    "              Orders\n",
    "    -returns all columns of all rows of the table\n",
    "STEPS:\n",
    "\n",
    "1-Import packages and functions\n",
    "2-Create the database engine\n",
    "3-Connect to the engine\n",
    "4-Query the database\n",
    "5-Save query results to a DataFrame\n",
    "6-Close the connection\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "engine = create_engine('sqlite:///Northwind.sqlite')\n",
    "con = engine.connect()\n",
    "rs = con.execute(\"SELECT * FROM Orders\")\n",
    "df = pd.DataFrame(rs.fetchall())\n",
    "df.columns = rs.keys()\n",
    "con.close()\n",
    "\n",
    "**Using the context manager**\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "engine = create_engine('sqlite:///Northwind.sqlite')\n",
    "with engine.connect() as con:\n",
    "rs = con.execute(\"SELECT OrderID, OrderDate, ShipName FROM Order\n",
    "df = pd.DataFrame(rs.fetchmany(size=5))\n",
    "df.columns = rs.keys()\n",
    "\n",
    "~conditional~\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT * FROM Employee **WHERE EmployeeId >= 6**\")\n",
    "    df = pd.DataFrame(rs.fetchall())\n",
    "    df.columns = rs.keys()\n",
    "~order by~\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT * FROM Employee **ORDER BY BirthDate**\")\n",
    "    df = pd.DataFrame(rs.fetchall())\n",
    "    \n",
    "    \n",
    "df=pd.read_sql_query(\"SELECT * FROM Orders\", engine)\n",
    "\n",
    "**ADVANCED QUERYING**\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "engine = create_engine('sqlite:///Northwind.sqlite')\n",
    "df = pd.read_sql_query(\"SELECT OrderID, CompanyName FROM Orders\n",
    "INNER JOIN Customers on Orders.CustomerID = Customers.CustomerID\", engine)\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOW TO IMPORT DATA FROM WEB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~urlopen() accepts URLs instead of file names.\n",
    "    from urllib.request import urlretrieve\n",
    "    url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'\n",
    "    urlretrieve(url, 'winequality-white.csv') -->to save the file locally\n",
    "\n",
    "\n",
    "URL= Uniform/Universial Resource Locator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTTP requests to import files from the web\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "protocol identifier- http: hyperText Trtansfer Protocol\n",
    "    each time you enter a website, you send a HTTP request to a server.=get request.urlretrieve()\n",
    "    resource name : datacamp.com\n",
    "\n",
    "~HTML: HyperTextMarkupLanguage\n",
    "\n",
    "**from urllib.request import urlopen, Request**\n",
    "url =\"https://www.wikipedia.org/\"\n",
    "request = Request(url)\n",
    "response = urlopen(request)\n",
    "html = response.read()\n",
    "response.close()\n",
    "\n",
    "**import requests**\n",
    "url =\"https://www.wikipedia.org/\"\n",
    "r = requests.get(url)\n",
    "text = r.text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **STRUCTURE DATA**\n",
    " -has pre-defined data model, or \n",
    " -organized in a defined manner\n",
    " **UNSTRUCTURE DATA**\n",
    " -neither of these properties\n",
    " \n",
    " **from bs4 import BeautifulSoup**\n",
    " import requests\n",
    " url= 'asdasdasasdadaadasd/dasda/BeautifulSoup/'\n",
    " r=requests.get(url)\n",
    " html_doc=r.text\n",
    " soup=BeautifulSoup(html_doc)\n",
    " \n",
    " print(soup.title)\n",
    " print(soup.get_text())\n",
    " \n",
    " for link in soup.find_all('a'):\n",
    "     print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIs and JSONs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**JSONS**\n",
    " JavaScript Object Notation.\n",
    " Real=time server to browser communication.\n",
    " it is human readable.\n",
    " ~it is natural to store them in a dict.\n",
    "  **import json\n",
    "  with open('snakes.json','r') as json_file:\n",
    "      json_data=json.load(json_file)**\n",
    "      type(json_data)==dict\n",
    " for key,value in json_data.items():\n",
    "     print(key+ ':' + value)\n",
    "     \n",
    "**API= Application Programming Interface**\n",
    " ~ BUILDING AND INTERACTING WITH SOFTWARE APPLICATIONS\n",
    " ~ bunch of code allows two software programs to communicate with each other. \n",
    " **import requests**\n",
    " url = 'http://www.omdbapi.com/?apikey=72bc447a&t=the+social+network'\n",
    " r=requests.get(url)\n",
    " json_data=r.json()\n",
    " for key,value in json_data.items():\n",
    "     print(key+ ':' + value)\n",
    "     \n",
    "     \n",
    " t=hackers : query string    \n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Twitter API and Authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Tweepy: Authentication handler**\n",
    "tw_auth.py\n",
    "import tweepy, json\n",
    "access_token =\n",
    "\"...\"\n",
    "access_token_secret =\n",
    "\"...\"\n",
    "consumer_key =\n",
    "\"...\"\n",
    "consumer_secret =\n",
    "\"...\"\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tweepy: dene stream listener class**\n",
    "st_class.py\n",
    "class MyStreamListener(tweepy.StreamListener):\n",
    "def __init__(self, api=None):\n",
    "super(MyStreamListener, self).__init__()\n",
    "self.num_tweets = 0\n",
    "self.file = open(\"tweets.txt\", \"w\")\n",
    "def on_status(self, status):\n",
    "tweet = status._json\n",
    "self.file.write(json.dumps(tweet) + '\\\\n')\n",
    "tweet_list.append(status)\n",
    "self.num_tweets += 1\n",
    "if self.num_tweets < 100:\n",
    "return True\n",
    "else:\n",
    "return False\n",
    "self.file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Streaming object and authenticate**\n",
    "l = MyStreamListener()\n",
    "stream = tweepy.Stream(auth, l)\n",
    "This line filters Twitter Streams to capture data by keywords:\n",
    "stream.filter(track=['apples','oranges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
